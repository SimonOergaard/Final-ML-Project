{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import fft\n",
    "import wave\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as signal\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/User/Desktop/AppStat/MachineLearning/AppliedML2024/final_project/data/nsynth-valid/audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_in_dir(directory):\n",
    "    filenames = os.listdir(directory)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "def audio_to_waveform(audio):\n",
    "    waveform, sr = librosa.load(audio, sr=None)\n",
    "    return waveform, sr\n",
    "\n",
    "def waveform_to_spectogram(waveform):\n",
    "    if waveform.dtype != np.float32:\n",
    "        waveform = waveform.astype(np.float32)\n",
    "    spectrogram = librosa.stft(waveform)\n",
    "    return np.abs(spectrogram)\n",
    "\n",
    "def pad_spectrogram(spec, target_shape):\n",
    "    padded_spec = np.zeros(target_shape,dtype = np.float32)\n",
    "    min_shape = np.minimum(target_shape, spec.shape)\n",
    "    padded_spec[:min_shape[0], :min_shape[1]] = spec[:min_shape[0], :min_shape[1]]\n",
    "    return padded_spec\n",
    "\n",
    "def spectogram_to_audio(spectrogram, sr,output_wav):\n",
    "    waveform = librosa.istft(spectrogram)\n",
    "    waveform = np.nan_to_num(waveform)\n",
    "    waveform = waveform/np.max(np.abs(waveform))\n",
    "    return write(output_wav, sr, (waveform*32767).astype(np.int16))\n",
    "\n",
    "def normalize_spectrogram(spectrogram):\n",
    "    min_val = np.min(spectrogram)\n",
    "    max_val = np.max(spectrogram)\n",
    "    normalized_spectrogram = (spectrogram - min_val) / (max_val - min_val + 1e-6)\n",
    "    return normalized_spectrogram\n",
    "\n",
    "def pick_samples_and_classify(arrays):\n",
    "    #Picks a random number of samples, and returns their filepath and label\n",
    "    instruments = []\n",
    "    #pick at minimum two instruments\n",
    "    number_of_instruments = 2\n",
    "    np.random.randint(2, len(arrays) + 1)\n",
    "    labels = np.zeros(len(arrays))\n",
    "    already_picked = []\n",
    "\n",
    "    while len(instruments) < number_of_instruments:\n",
    "        random_pick = np.random.randint(0, len(arrays))\n",
    "        if random_pick in already_picked:\n",
    "            break\n",
    "        else: \n",
    "            already_picked.append(random_pick)\n",
    "            pick = np.random.choice(arrays[random_pick], 1)\n",
    "            instruments.append(pick)\n",
    "            labels[random_pick] = 1\n",
    "\n",
    "    return instruments, labels\n",
    "\n",
    "#read the filenames, and add their data to 5 lists\n",
    "def add_waveform_to_list(filenames):\n",
    "    waveforms = []\n",
    "    for filename in filenames:\n",
    "        waveform, params = audio_to_waveform(path + filename[0])\n",
    "        waveforms.append(waveform)\n",
    "    return waveforms\n",
    "\n",
    "def find_longest_array(arrays):\n",
    "    longest = 0\n",
    "    for array in arrays:\n",
    "        if len(array) > longest:\n",
    "            longest = len(array)\n",
    "    return longest\n",
    "\n",
    "def combine_waveforms(waveforms):\n",
    "    normalization = 1 / len(waveforms)\n",
    "    # changed to be equal to the length of the longest waveform\n",
    "    out = np.zeros(find_longest_array(waveforms), dtype=np.float32)\n",
    "    for w in waveforms:\n",
    "        out += w.astype(np.float32) * normalization\n",
    "    return out\n",
    "\n",
    "# Function to plot spectrogram using librosa\n",
    "def plot_spectrogram(waveform, sr, title):\n",
    "    # Compute the STFT\n",
    "    D = librosa.stft(waveform)\n",
    "    # Convert to magnitude\n",
    "    D_mag = np.abs(D)\n",
    "    print(D_mag.shape)\n",
    "    # Plot the spectrogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(D_mag, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "\n",
    "def plot_separated_spectrograms(output_tensor, sr):\n",
    "    for i, spec in enumerate(output_tensor):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        librosa.display.specshow(librosa.amplitude_to_db(spec, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.ylabel('Frequency [Hz]')\n",
    "        plt.xlabel('Time [sec]')\n",
    "        plt.title(f'Separated Spectrogram {i + 1}')\n",
    "        plt.show()\n",
    "\n",
    "def torch_normalize(spectogram):\n",
    "    min_val = torch.min(spectogram)\n",
    "    max_val = torch.max(spectogram)\n",
    "    normalized_spectogram = (spectogram - min_val) / (max_val - min_val)\n",
    "    return normalized_spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nu_gen_spectro(N, target_shape=(1025, 126)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    original_labels = []\n",
    "    #instrument_list = [bass, guitar, flutes, keyboards]\n",
    "    instrument_list = [bass_electronic, flute_acoustic]\n",
    "\n",
    "    for i in range(N):\n",
    "        paths, label = pick_samples_and_classify(instrument_list)\n",
    "        original_labels.append(label)\n",
    "        waveforms = add_waveform_to_list(paths)\n",
    "        mixed_waveform = combine_waveforms(waveforms)\n",
    "        \n",
    "        #audio = audio_to_waveform(mixed_waveform)\n",
    "        mixed_spectro = waveform_to_spectogram(mixed_waveform)\n",
    "        #mixed_spectro = audio_to_spectrogram(mixed_waveform)\n",
    "        mixed_spectro_padded = pad_spectrogram(mixed_spectro, target_shape)\n",
    "        mixed_spectro_normalized = normalize_spectrogram(mixed_spectro_padded)\n",
    "        \n",
    "        inter_waveforms = []\n",
    "        inst_i = 0\n",
    "        for n, i in enumerate(label):\n",
    "            if i == 1:\n",
    "                spectro = waveform_to_spectogram(waveforms[inst_i])\n",
    "                spectro_padded = pad_spectrogram(spectro, target_shape)\n",
    "                spectro_normalized = normalize_spectrogram(spectro_padded)\n",
    "                inter_waveforms.append(spectro_normalized)\n",
    "                inst_i += 1\n",
    "\n",
    "            if i == 0:\n",
    "                inter_waveforms.append(np.zeros(target_shape))\n",
    "\n",
    "        data.append(mixed_spectro_normalized)\n",
    "        #inter_waveforms.append(np.zeros(target_shape))\n",
    "        labels.append(inter_waveforms)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    return data, np.array(labels), np.array(original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = read_files_in_dir(path)\n",
    "#pianos = [filename for filename in filenames if \"piano\" in filename] #empty\n",
    "bass = [filename for filename in filenames if \"bass\" in filename]\n",
    "guitar = [filename for filename in filenames if \"guitar\" in filename]\n",
    "#drum = [filename for filename in filenames if \"drum\" in filename] #empty\n",
    "flutes = [filename for filename in filenames if \"flute\" in filename]\n",
    "keyboards = [filename for filename in filenames if \"keyboard\" in filename] \n",
    "guitar_acoustic = [filename for filename in filenames if \"guitar_acoustic\" in filename]\n",
    "flute_acoustic = [filename for filename in filenames if \"flute_acoustic\" in filename]\n",
    "bass_electronic = [filename for filename in filenames if \"bass_electronic\" in filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (1025, 126)\n",
    "N = 3000  # Number of samples\n",
    "\n",
    "\n",
    "data, labels, original_labels = nu_gen_spectro(N, target_shape)\n",
    "\n",
    "print(np.shape(data))  # Should be (N, 1025, 128)\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=2):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Define the contracting/downsampling path\n",
    "        self.conv1 = nn.Conv2d(input_channels, 8, (3,125), padding=(1,62)) # Maybe different kernel and padding.\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Add transpose convolutional layers for upsampling\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv8 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv9 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv10 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.conv11 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.upconv5 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "        self.conv12 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        self.upconv6 = nn.ConvTranspose2d(16, 8, 2, stride=2)\n",
    "        self.conv13 = nn.Conv2d(16, 8, 3, padding=1)\n",
    "        self.final_conv = nn.Conv2d(8, output_channels, 3,padding = 1)\n",
    "    def forward(self, x):\n",
    "        # Contracting path\n",
    "        x = self.dropout(x)\n",
    "        x1 = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x2 = self.pool(nn.functional.relu(self.bn2(self.conv2(x1))))\n",
    "        x3 = self.pool(nn.functional.relu(self.bn3(self.conv3(x2))))\n",
    "        x4 = self.pool(nn.functional.relu(self.bn4(self.conv4(x3))))\n",
    "        x5 = self.pool(nn.functional.relu(self.bn5(self.conv5(x4))))\n",
    "        x6 = self.pool(nn.functional.relu(self.bn6(self.conv6(x5))))\n",
    "        x7 = self.pool(nn.functional.relu(self.bn7(self.conv7(x6))))\n",
    "        # Print sizes for debugging\n",
    "        #print(f\"x1 shape: {x1.shape}\")\n",
    "        #print(f\"x2 shape: {x2.shape}\")\n",
    "        #print(f\"x3 shape: {x3.shape}\")\n",
    "        #print(f\"x4 shape: {x4.shape}\")\n",
    "        #print(f\"x5 shape: {x5.shape}\")\n",
    "        #print(f\"x6 shape: {x6.shape}\")\n",
    "        #print(f\"x7 shape: {x7.shape}\")\n",
    "        x = self.upconv1(x7)\n",
    "        x = F.interpolate(x, size=(x6.size(2), x6.size(3)), mode='nearest')\n",
    "    \n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x6], dim=1)\n",
    "        #print(f\"x shape after concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv8(x))\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = F.interpolate(x, size=(x5.size(2), x5.size(3)), mode='nearest')\n",
    "                #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x5], dim=1)\n",
    "        #print(f\"x shape after concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv9(x))\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = F.interpolate(x, size=(x4.size(2), x4.size(3)), mode='nearest')\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv10(x))\n",
    "        \n",
    "        x = self.upconv4(x)\n",
    "        x = F.interpolate(x, size=(x3.size(2), x3.size(3)), mode='nearest')\n",
    "        #print(f\"x2_interp shape: {x3.shape}\")\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "    \n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv11(x))\n",
    "\n",
    "        x = self.upconv5(x)\n",
    "        x = F.interpolate(x, size=(x2.size(2), x2.size(3)), mode='nearest')\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        #print(f\"x shape before concatenation: {x.shape}\")\n",
    "        x = nn.functional.relu(self.conv12(x))\n",
    "        x = self.upconv6(x)\n",
    "        #print(f'upconv6 shape: {x.shape}')\n",
    "        x = F.interpolate(x, size=(x1.size(2), x1.size(3)), mode='nearest')\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = F.relu(self.conv13(x))\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model = UNet(input_channels=1, output_channels=2)\n",
    "#input_tensor = torch.randn(1, 1, 128, 128)  # Adjust size as necessary\n",
    "#y_pred = model(input_tensor)\n",
    "#from torchviz import make_dot\n",
    "# Visualize the model\n",
    "#make_dot(y_pred, params=dict(model.named_parameters())).render(\"unet\", format=\"png\")\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "#data = data[..., np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(y_train.shape)\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "# Set all values in x_train and x_test to be 0 for values lower than e-2\n",
    "X_train[X_train < 1e-2] = 0\n",
    "X_test[X_test < 1e-2] = 0\n",
    "# Create the datasets and dataloaders\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "#print(train_dataset[0][0].shape)\n",
    "\n",
    "# Get a batch of data\n",
    "inputs, targets = next(iter(train_loader))\n",
    "# Print the shape of the inputs\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different torch loss functions\n",
    "class AbsoluteLogDifferenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AbsoluteLogDifferenceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Ensure that we avoid log of zero by adding a small epsilon\n",
    "        epsilon = 1e-6\n",
    "        # if y_pred is negative set it to 0\n",
    "        y_pred = torch.clamp(y_pred, min=0)\n",
    "        # if y_pred is infite set it to 0\n",
    "        log_diff0 = torch.abs(torch.log(y_pred[:,0,:] + epsilon) - torch.log(y_true[:,0,:] + epsilon))\n",
    "        log_diff1 = torch.abs(torch.log(y_pred[:,1,:] + epsilon) - torch.log(y_true[:,1,:] + epsilon))\n",
    "        \n",
    "        log_diff = log_diff0 + log_diff1\n",
    "        #print(log_diff)\n",
    "        regulator = 1\n",
    "        #penalty = torch.abs(torch.log(y_pred[:,0,:])) + torch.abs(torch.log(y_pred[:,1,:])) - (torch.abs(torch.log(y_true[:,0,:])) + torch.abs(torch.log(y_true[:,1,:])))\n",
    "        penalty = torch.abs(y_pred[:,2,:])\n",
    "        #print(penalty)\n",
    "        #+ LA.matrix_norm(regulator*penalty, ord = 'fro')\n",
    "        # Use the L2 norm to calulate the loss\n",
    "        loss = torch.mean(log_diff) + torch.mean(regulator*penalty)\n",
    "        #print(np.shape(loss))\n",
    "        return loss\n",
    "\n",
    "class KLdivLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLdivLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        kl_div0 = F.kl_div(y_pred[:,0,:], y_true[:,0,:], reduction='batchmean')\n",
    "        kl_div1 = F.kl_div(y_pred[:,1,:], y_true[:,1,:], reduction='batchmean')\n",
    "        kl_div = torch.sum(kl_div0 + kl_div1)\n",
    "        return kl_div\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true,X):\n",
    "        l1_loss0 = F.l1_loss(y_pred[:,0,:], y_true[:,0,:])\n",
    "        l1_loss1 = F.l1_loss(y_pred[:,1,:], y_true[:,1,:])\n",
    "        #y_pred = torch.clamp(y_pred, min=0)\n",
    "        sq_X = torch.squeeze(X)\n",
    "        summed_spectogram = torch.sum(y_pred, dim=1)\n",
    "        diff = torch.abs(torch_normalize(torch.squeeze(summed_spectogram)) - torch_normalize(sq_X))\n",
    "        #penalty = torch.abs((y_pred[:,0,:]) + torch.abs(y_pred[:,1,:])) - (torch.abs(y_true[:,0,:]) + torch.abs(y_true[:,1,:]))\n",
    "        l1_loss = torch.mean(l1_loss0 + l1_loss1) + torch.mean(diff)\n",
    "        return l1_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_channels = X_train.shape[1]  # Get the number of input channels from the data\n",
    "print(input_channels)\n",
    "model = UNet(input_channels=input_channels,output_channels=2).to(device, dtype=torch.float32)\n",
    "criterion = L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "best_loss = np.inf\n",
    "patience = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n",
    "        \n",
    "        #print(f\"Shape of x : {X.shape},Shape of y {y.shape}\")\n",
    "        optimizer.zero_grad() \n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X.size(0) # Accumulate loss\n",
    "        #print(f\"Loss: {loss.item()}\")\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    #Implement early stopping\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y, X)\n",
    "            \n",
    "            val_loss += 4*loss.item() * X.size(0)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience > 5:\n",
    "        break\n",
    "    \n",
    "\n",
    "    epoch_loss = running_loss\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "   \n",
    "torch.save(model.state_dict(), 'your_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a random waveform\n",
    "instrument_list = [bass_electronic, flute_acoustic]\n",
    "#Pick a random sample of each instrument\n",
    "filepaths, labels = pick_samples_and_classify(instrument_list)\n",
    "\n",
    "#Extract .wav data into to a list using librosa\n",
    "waveforms = add_waveform_to_list(filepaths)\n",
    "print(waveforms[0])\n",
    "#Combine the waveforms\n",
    "combined_waveform = combine_waveforms(waveforms)\n",
    "\n",
    "# Convert the combined waveform to a spectrogram\n",
    "spectrogram = waveform_to_spectogram(combined_waveform)\n",
    "# Set all values lower than e-2 to 0\n",
    "spectogram_0 = np.where(spectrogram < 1e-2, 0, spectrogram)\n",
    "# Normalize the spectrogram\n",
    "normalized_spectrogram = normalize_spectrogram(spectrogram)\n",
    "normalized_spectrogram_0 = normalize_spectrogram(spectogram_0)\n",
    "# Convert the normalized spectrogram back to audio and save it\n",
    "spectogram_to_audio(normalized_spectrogram, 16000, \"combo1.wav\")\n",
    "spectogram_to_audio(normalized_spectrogram_0, 16000, \"combo2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the spectrograms for the individual waveforms\n",
    "sr = 16000  # Assuming a sample rate of 16kHz\n",
    "\n",
    "plot_spectrogram(waveforms[0], sr, 'Spectrogram of First Waveform')\n",
    "plot_spectrogram(waveforms[1], sr, 'Spectrogram of Second Waveform')\n",
    "\n",
    "# If you want to plot the spectrogram of the combined waveform as well\n",
    "plot_spectrogram(combined_waveform, sr, 'Spectrogram of Combined Waveform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import wave\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "# Initialize and load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(input_channels=1,output_channels=2).to(device, dtype=torch.float32)\n",
    "model.load_state_dict(torch.load('your_model_weights.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with wave.open('combo1.wav', 'rb') as wf:\n",
    "    input_audio = wf.readframes(-1)\n",
    "    input_sr = wf.getframerate()\n",
    "\n",
    "#Load the combined audio waveform\n",
    "combined_waveform, input_sr = librosa.load('combo1.wav', sr=None)\n",
    "\n",
    "# Compute spectrogram using scipy signal\n",
    "#f, t, Zxx = signal.spectrogram(combined_waveform, fs=input_sr, nperseg=256)\n",
    "spectrogram = waveform_to_spectogram(combined_waveform)\n",
    "\n",
    "# Convert spectrogram to PyTorch tensor\n",
    "input_tensor = torch.tensor(spectrogram, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device, dtype=torch.float32)  # Add batch and channel dimensions\n",
    "#print(input_tensor[0])\n",
    "# Pass input through the model to get predictions\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# Convert output tensor to numpy array and save as audio files\n",
    "output_tensor = output_tensor.squeeze().cpu().numpy()\n",
    "#print(output_tensor[0])\n",
    "# Plot the separated spectrograms\n",
    "plot_separated_spectrograms(output_tensor, input_sr)\n",
    "output_folder = \"output_audio\"\n",
    "#print(output_tensor)\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Convert each source back from spectrogram to audio and save\n",
    "for i, source in enumerate(output_tensor):\n",
    "    # Convert the spectrogram back to audio\n",
    "\n",
    "    # Save as .wav file\n",
    "    output_filepath = os.path.join(output_folder, f'source_{i}.wav')\n",
    "    spectogram_to_audio(source, input_sr, output_filepath)\n",
    "    print(f\"Saved {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the spectgram of the sum separated audio files subtracted from the original audio file\n",
    "\n",
    "# Load the original audio file\n",
    "original_waveform, sr = librosa.load('combo1.wav', sr=None)\n",
    "\n",
    "# Load the separated audio files\n",
    "separated_waveforms = []\n",
    "for i in range(2):\n",
    "    separated_waveform, sr = librosa.load(f'output_audio/source_{i}.wav', sr=None)\n",
    "    separated_waveforms.append(separated_waveform)\n",
    "\n",
    "# Sum the separated waveforms\n",
    "sum_separated_waveform = np.sum(separated_waveforms, axis=0)\n",
    "\n",
    "# Subtract the sum of the separated waveforms from the original waveform\n",
    "diff_waveform = original_waveform - sum_separated_waveform\n",
    "\n",
    "# Compute the spectrogram of the difference waveform\n",
    "diff_spectrogram = waveform_to_spectogram(diff_waveform)\n",
    "\n",
    "# Plot the spectrogram of the difference waveform\n",
    "\n",
    "plot_spectrogram(diff_waveform, sr, 'Spectrogram of Difference Waveform')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
